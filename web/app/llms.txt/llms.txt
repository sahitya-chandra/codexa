# Codexa

Codexa is a powerful CLI tool that ingests your codebase and allows you to ask questions about it using Retrieval-Augmented Generation (RAG).

## Features

- **Privacy-First**: All data processing happens locally by default.
- **Fast & Efficient**: Local embeddings and optimized vector search.
- **Multiple LLM Support**: Works with Ollama (local) and Groq (cloud).
- **Local Storage**: SQLite database for embeddings and context.
- **Smart Chunking**: Intelligent code splitting with configurable overlap.
- **Session Management**: Maintain conversation context across queries.
- **Streaming Output**: Real-time response streaming for better UX.
- **Multiple File Types**: Supports TypeScript, JavaScript, Python, Go, Rust, Java, and more.
- **Highly Configurable**: Fine-tune chunking, retrieval, and model parameters.
- **Zero Setup**: Works out of the box with sensible defaults.

## Installation

Install Codexa globally using npm:

```bash
npm install -g codexa
```

Or install via Homebrew (macOS):

```bash
brew tap sahitya-chandra/codexa
brew install codexa
```

## Quick Start

1. **Initialize Codexa:**
   ```bash
   codexa init
   ```

2. **Ingest your codebase:**
   ```bash
   codexa ingest
   ```

3. **Ask questions:**
   ```bash
   codexa ask "How does the authentication flow work?"
   ```

## Configuration

Codexa uses a `.codexarc.json` file in your project root.

### LLM Setup

- **Groq (Cloud - Recommended)**: Set `GROQ_API_KEY` environment variable.
- **Ollama (Local)**: Install Ollama and pull a model (e.g., `qwen2.5:3b-instruct`).

### Example Configuration (Groq)

```json
{
  "modelProvider": "groq",
  "model": "llama-3.1-8b-instant",
  "embeddingProvider": "local",
  "embeddingModel": "Xenova/all-MiniLM-L6-v2"
}
```

## Commands

- `codexa init`: Create configuration file.
- `codexa ingest`: Index codebase.
- `codexa ask <question>`: Ask questions about the codebase.

## License

MIT License
