---
title: Introduction
description: Codexa is a powerful CLI tool that ingests your codebase and allows you to ask questions about it using Retrieval-Augmented Generation (RAG).
---

## Features

- ğŸ”’ **Privacy-First**: All data processing happens locally by default
- âš¡ **Fast & Efficient**: Local embeddings and optimized vector search
- ğŸ¤– **Multiple LLM Support**: Works with Groq (cloud)
- ğŸ’¾ **Local Storage**: SQLite database for embeddings and context
- ğŸ¯ **Smart Chunking**: Intelligent code splitting with configurable overlap
- ğŸ”„ **Session Management**: Maintain conversation context across queries
- ğŸ“Š **Streaming Output**: Real-time response streaming for better UX
- ğŸ¨ **Multiple File Types**: Supports TypeScript, JavaScript, Python, Go, Rust, Java, and more
- âš™ï¸ **Highly Configurable**: Fine-tune chunking, retrieval, and model parameters
- ğŸš€ **Zero Setup**: Works out of the box with sensible defaults

## How It Works

Codexa uses Retrieval-Augmented Generation (RAG) to answer questions about your codebase:

### 1. Ingestion Phase

When you run `codexa ingest`:

1. **File Discovery**: Scans your repository using glob patterns (`includeGlobs`/`excludeGlobs`)
2. **Code Chunking**: Splits files into manageable chunks with configurable overlap
3. **Embedding Generation**: Creates vector embeddings for each chunk using local transformers
4. **Storage**: Stores chunks and embeddings in a SQLite database (`.codexa/index.db`)

### 2. Query Phase

When you run `codexa ask`:

1. **Question Embedding**: Converts your question into a vector embedding
2. **Vector Search**: Finds the most similar code chunks using cosine similarity
3. **Context Retrieval**: Selects top-K most relevant chunks as context
4. **LLM Generation**: Sends question + context to your configured LLM
5. **Response**: Returns an answer grounded in your actual codebase

### Benefits

- **Privacy**: All processing happens locally by default
- **Speed**: Local embeddings and vector search are very fast
- **Accuracy**: Answers are based on your actual code, not generic responses
- **Context-Aware**: Understands relationships across your codebase

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding      â”‚â”€â”€â”€â”€â–¶â”‚   Vector     â”‚
â”‚  Generation     â”‚     â”‚   Search     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Context    â”‚
                        â”‚   Retrieval  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQLite DB     â”‚â—€â”€â”€â”€â”€â”‚   LLM        â”‚
â”‚   (Chunks +     â”‚     â”‚   (Groq)     â”‚
â”‚   Embeddings)   â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Answer     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**
- **Chunker**: Splits code files into semantic chunks
- **Embedder**: Generates vector embeddings (local transformers)
- **Retriever**: Finds relevant chunks using vector similarity
- **LLM Client**: Generates answers (Groq cloud)
- **Database**: SQLite for storing chunks and embeddings
